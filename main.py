# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MHpKpATeXqG99SXScrXjXP0BT6XslY5w
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

from google.colab import output
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
!pip install scipy
from scipy.io import arff

#rialto_stream = pd.read_csv("/content/drive/My Drive/Rialto.csv", header=None)
#posture_stream = pd.read_csv("/content/drive/My Drive/Posture.csv") #Error tokenizing data. C error: Expected 4 fields in line 2000, saw 7
#posture_stream = pd.concat([posture_stream.iloc[:, :3], posture_stream.iloc[:,-1]], axis=1)
#posture_stream = posture_stream[posture_stream["label"] != 8]
#pokerhand_stream = pd.read_csv("/content/drive/My Drive/PokerHand.csv", header=None)
#noaa_stream = pd.read_csv("/content/drive/My Drive/NOAA.csv", header=None)
#smartmeter_stream = pd.read_csv("/content/drive/My Drive/SmartMeter_LADPU.csv") #Error tokenizing data. C error: Expected 97 fields in line 300, saw 193
#smartmeter_stream = pd.concat([smartmeter_stream.iloc[:, :96], smartmeter_stream.iloc[:,-1]], axis=1)
forestcover_stream = pd.read_csv("/content/drive/My Drive/ForestCoverType.csv", header=None)
#electricity = pd.read_csv("/content/drive/My Drive/results_electricity.csv", header=None)

!pip install tabpfn

from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import train_test_split

from tabpfn import TabPFNClassifier

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize a classifier
classifier = TabPFNClassifier()
classifier.fit(X_train, y_train)

# Predict probabilities
prediction_probabilities = classifier.predict_proba(X_test)
print("ROC AUC:", roc_auc_score(y_test, prediction_probabilities[:, 1]))

# Predict labels
predictions = classifier.predict(X_test)
print("Accuracy", accuracy_score(y_test, predictions))

!pip install river

import random
import river

import numpy as np
from pathlib import Path
import pandas as pd
import time

import matplotlib.pyplot as plt
import datetime as dt

from river import stream
from river import base

import graphviz

import pandas as pd
import os

import warnings

warnings.filterwarnings("ignore")

class SlidingWindowSampler:
    def __init__(self, short_memory_size, long_memory_size, columns, T_warm):
        self.short_memory_size = short_memory_size
        self.long_memory_size = long_memory_size
        self.short_memory = []  # Short-term memory
        self.long_memory = []  # Long-term memory
        self.class_counts = {}  # Track counts per class in long-term memory
        self.columns = columns
        self.T_warm = T_warm
        self.t_warm = 0

    def add(self, X_instance, y_instance):
        """
        Add an instance to short-term memory first. If short-term memory exceeds
        its max size, the oldest instance is considered for long-term memory.
        """
        if len(self.short_memory) >= self.short_memory_size:
            oldest_X, oldest_y = self.short_memory.pop(0)  # Remove oldest from short-term
            self._consider_for_long_term(oldest_X, oldest_y)

        self.short_memory.append((X_instance, y_instance))

    def _consider_for_long_term(self, X_instance, y_instance):
        """
        Add the instance to long-term memory directly, and manage memory if it exceeds capacity.
        """
        # Always add the instance to long-term memory
        self.long_memory.append((X_instance, y_instance))
        self.class_counts[y_instance] = self.class_counts.get(y_instance, 0) + 1

        # If memory exceeds capacity, remove the oldest from overrepresented class
        if len(self.long_memory) > self.long_memory_size:
            self._remove_oldest_from_overrepresented()

    def _remove_oldest_from_overrepresented(self):
        """
        Remove the oldest instance from the most overrepresented class in long-term memory.
        In case of a tie, remove the oldest instance among the classes with the highest count.
        """
        if not self.long_memory:
            return

        # Find the class(es) with the maximum count in long-term memory
        max_count = max(self.class_counts.values())
        overrepresented_classes = [class_label for class_label, count in self.class_counts.items() if count == max_count]

        # Identify the oldest instance among these overrepresented classes
        oldest_instance = None
        for i, (_, y) in enumerate(self.long_memory):
            if y in overrepresented_classes:
                oldest_instance = i
                break

        # Remove the identified oldest instance
        if oldest_instance is not None:
            _, y = self.long_memory.pop(oldest_instance)
            self.class_counts[y] -= 1
            if self.class_counts[y] == 0:
                del self.class_counts[y]

    def get_memory_as_dataframe(self):
        """
        Convert both short-term and long-term memory into a DataFrame.
        """
        combined_memory = self.short_memory + self.long_memory
        X_data = [X for X, _ in combined_memory]
        y_data = [y for _, y in combined_memory]
        X_df = pd.DataFrame(X_data, columns=self.columns)
        y_df = pd.DataFrame(y_data, columns=["target"])
        return pd.concat([X_df, y_df], axis=1)

# Set memory sizes
short_memory_size = 750
long_memory_size = 250  # Example size, adjust as needed
columns = forestcover_stream.columns[:-1]
T_warm = 100  # Set the warm-up time as needed
window_sampler = SlidingWindowSampler(short_memory_size, long_memory_size, columns, T_warm)

i = -1
correct = 0
wrong = 0
start = time.time()
results = []

csv_filename = "/content/drive/My Drive/forestcover.csv"
if not os.path.exists(csv_filename):
    pd.DataFrame(columns=["Instance", "Correct", "Wrong", "Time"]).to_csv(csv_filename, index=False)

# Stream processing
for X_test_dict, y_test_dict in stream.iter_pandas(forestcover_stream.iloc[:, :-1], forestcover_stream.iloc[:, -1]):
    i += 1
    X_test = forestcover_stream.iloc[i:i+1, :-1]
    y_test = forestcover_stream.iloc[i, -1]

    # Warm-up phase
    if i > (T_warm - 1):
        if window_sampler.t_warm > T_warm:
            # Learn in-context with LTM (Long-Term Memory)
            training_data_df = window_sampler.get_memory_as_dataframe()
            X_train = training_data_df.iloc[:, :-1]
            y_train = training_data_df.iloc[:, -1]
            classifier.fit(X_train, y_train)
            y_eval = classifier.predict(X_test)

            if y_eval == y_test:
                correct += 1
            else:
                wrong += 1
        window_sampler.t_warm += 1

    window_sampler.add(X_test.iloc[0].tolist(), y_test)
    results.append([i, correct, wrong, time.time() - start])

    if i % 1000 == 0:
        results_df = pd.DataFrame(results, columns=["Instance", "Correct", "Wrong", "Time"])
        results_df.to_csv(csv_filename, mode="a", index=False, header=False)
        results = []
        print(i, 'Correct', correct, 'Wrong', wrong)

print('Time', time.time() - start)

ARF = pd.read_csv('/content/drive/My Drive/ARP.txt', usecols=['classified instances', 'classifications correct (percent)'])
SRP = pd.read_csv('/content/drive/My Drive/SRP.txt', usecols=['classified instances', 'classifications correct (percent)'])
BOLE = pd.read_csv('/content/drive/My Drive/BOLE.txt', usecols=['classified instances', 'classifications correct (percent)'])
LTM = pd.read_csv('/content/drive/My Drive/LTM.csv', sep=';', usecols=[0, 1])
LTM.columns = ['classified instances', 'classifications correct (percent)']

LTM['classifications correct (percent)'] = LTM['classifications correct (percent)'].apply(
    lambda x: x * 100 if x < 1.1 else x
)

!pip install SciencePlots

import pandas as pd
import matplotlib.pyplot as plt
import scienceplots

plt.style.use(['science', 'no-latex', 'grid', 'ieee'])

plt.figure(figsize=(14, 3))

plt.plot(LTM.iloc[:, 0], LTM.iloc[:, 1], label='LTM')
plt.plot(ARF.iloc[:, 0], ARF.iloc[:, 1], label='ARF')
plt.plot(SRP.iloc[:, 0], SRP.iloc[:, 1], label='SRP')
plt.plot(BOLE.iloc[:, 0], BOLE.iloc[:, 1], label='BOLE')

plt.xlabel('Instances')
plt.ylabel('Prequential Accuracy')
plt.legend()

save_path = '/content/drive/My Drive/plot.pdf'
plt.savefig(save_path, format="pdf", bbox_inches="tight")

plt.show()